# --- START OF FILE fine_tune_gpt.py ---
import logging
import sys
import os
import torch # Import torch

# Setup basic logging for the script
log_file = "fine_tune_gpt.log"
log_format = '%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
logging.basicConfig(level=logging.INFO, format=log_format, handlers=[
    logging.FileHandler(log_file, mode='w'),
    logging.StreamHandler(sys.stdout)
])
logger = logging.getLogger(__name__)

# Ensure project root is in path if running script directly
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir) # Assuming script is in root, adjust if needed
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    logger.info(f"Added project root to sys.path: {project_root}")


try:
    # Import necessary components AFTER potentially adding to path
    from config import MasterConfig as Config, TRAIN_DATA, GPT_SAVE_PATH # Get Config, loaded TRAIN_DATA, and save path
    from ai_modules import TransformerGPT, TRANSFORMERS_AVAILABLE # Import the correct GPT wrapper
except ImportError as e:
    logger.critical(f"Error importing necessary modules: {e}. Ensure paths are correct and libraries installed.")
    sys.exit(1)
except Exception as e:
    logger.critical(f"Unexpected error during imports: {e}")
    sys.exit(1)


def run_fine_tuning():
    logger.info("--- Starting GPT Fine-Tuning Script ---")

    if not TRANSFORMERS_AVAILABLE:
        logger.critical("'transformers' or 'datasets' library not found. Cannot fine-tune.")
        return

    if not TRAIN_DATA:
        logger.error("No training data loaded from config. Cannot fine-tune.")
        return

    try:
        # 1. Initialize the GPT Model Wrapper
        # This will load the base pre-trained model specified in config
        logger.info(f"Loading base model '{Config.NLP.HUGGINGFACE_MODEL}' for fine-tuning...")
        # Check if a fine-tuned model already exists, maybe load that instead?
        # For this script, let's assume we always start from the base model specified in config.
        # If you want to continue fine-tuning, you'd load from GPT_SAVE_PATH first.
        gpt_model_wrapper = TransformerGPT(model_name=Config.NLP.HUGGINGFACE_MODEL)

        # 2. Run the fine-tuning process
        # The train_model method handles data prep, Trainer setup, training, and saving
        logger.info(f"Starting fine-tuning with {len(TRAIN_DATA)} raw samples for {Config.NLP.TRAIN_EPOCHS} epochs.")
        gpt_model_wrapper.train_model(dataset=TRAIN_DATA, epochs=Config.NLP.TRAIN_EPOCHS)

        logger.info("--- GPT Fine-Tuning Script Finished ---")
        logger.info(f"Fine-tuned model saved in directory: {GPT_SAVE_PATH}")

    except Exception as e:
        logger.critical(f"An error occurred during the fine-tuning process: {e}", exc_info=True)

if __name__ == "__main__":
    # Check for GPU
    if torch.cuda.is_available():
        logger.info(f"CUDA available. Training on GPU: {torch.cuda.get_device_name(0)}")
    else:
        logger.warning("CUDA not available. Fine-tuning will run on CPU and may be very slow.")

    run_fine_tuning()

# --- END OF FILE fine_tune_gpt.py ---
